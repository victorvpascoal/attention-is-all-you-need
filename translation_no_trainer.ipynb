{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88844159-113c-4e10-abc2-80d998c7c016",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "import pickle\n",
    "import datetime\n",
    "\n",
    "from datasets import load_dataset\n",
    "from tokenizers import Tokenizer, ByteLevelBPETokenizer\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset, Sampler\n",
    "\n",
    "import sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "862467de-9d7f-49ae-a44f-51b4e8e94bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on the GPU if possible\n",
    "DEVICE = \"cuda\"\n",
    "\n",
    "VOCAB_SIZE = 37_000\n",
    "# For memory control (More than 99% of WMT14 EN-DE sequences have seq_len <= 500)\n",
    "MAX_SEQ_LEN = 500\n",
    "MAX_TOKENS_PER_BATCH = 10_000\n",
    "\n",
    "D_MODEL = 512\n",
    "# During warm-up 100 M tokens are consumed\n",
    "WARMUP_STEPS = 100_000_000 // MAX_TOKENS_PER_BATCH\n",
    "LABEL_SMOOTHING = 0.1\n",
    "\n",
    "# Base model is trained with 2.5 billion tokens\n",
    "# MAX_STEPS = 2_500_000_000 // MAX_TOKENS_PER_BATCH\n",
    "MAX_STEPS = 75_000\n",
    "EVAL_STEPS = 1_000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078601f3-f717-4bf0-9d86-9832268339ac",
   "metadata": {},
   "source": [
    "## 1. Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2879608-1cdd-40b4-b30d-2b99b8e2523a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading WMT4\n",
    "raw_dataset = load_dataset(\"wmt14\", \"de-en\")\n",
    "\n",
    "# Create BPE data for training\n",
    "if not os.path.exists(\"bpe_data/train.en\") or not os.path.exists(\"bpe_data/train.de\"):\n",
    "    os.makedirs(\"bpe_data\", exist_ok=True)\n",
    "    with open(\"bpe_data/train.en\", \"w\", encoding=\"utf-8\") as f_en, open(\"bpe_data/train.de\", \"w\", encoding=\"utf-8\") as f_de:\n",
    "        for example in raw_dataset[\"train\"]:\n",
    "            f_en.write(example[\"translation\"][\"en\"] + \"\\n\")\n",
    "            f_de.write(example[\"translation\"][\"de\"] + \"\\n\")\n",
    "\n",
    "# Train tokenizer with shared source and target sequences\n",
    "if not os.path.exists(\"bpe_data/tokenizer.json\"):    \n",
    "    tokenizer = ByteLevelBPETokenizer()\n",
    "    tokenizer.train([\"bpe_data/train.en\", \"bpe_data/train.de\"],\n",
    "                    vocab_size=VOCAB_SIZE,\n",
    "                    show_progress=True,\n",
    "                    special_tokens=[\"<PAD>\", \"<START>\", \"<END>\", \"<UNK>\"])\n",
    "    \n",
    "    tokenizer._tokenizer.post_processor = TemplateProcessing(\n",
    "        single=f\"<START>:0 $A:0 <END>:0\",\n",
    "        pair=f\"<START>:0 $A:0 <END>:0 <START>:1 $B:1 <END>:1\",\n",
    "        special_tokens=[\n",
    "            (\"<START>\", tokenizer.token_to_id(\"<START>\")),\n",
    "            (\"<END>\", tokenizer.token_to_id(\"<END>\")),\n",
    "        ],\n",
    "    )    \n",
    "    tokenizer.save(\"bpe_data/tokenizer.json\")\n",
    "\n",
    "tokenizer = Tokenizer.from_file(\"bpe_data/tokenizer.json\")\n",
    "tokenizer.enable_truncation(max_length=MAX_SEQ_LEN)\n",
    "\n",
    "PAD_ID = tokenizer.token_to_id(\"<PAD>\")\n",
    "START_ID = tokenizer.token_to_id(\"<START>\")\n",
    "END_ID = tokenizer.token_to_id(\"<END>\")\n",
    "UNK_ID = tokenizer.token_to_id(\"<UNK>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2047f3e-10df-4028-b056-2b70adfc91ee",
   "metadata": {},
   "source": [
    "### 2. Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3fa7c2e4-dfb3-4e69-b53c-22d5afa79721",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WMT14Dataset(Dataset):\n",
    "    def __init__(self, split=\"train\", tokenizer=None):\n",
    "        super().__init__()\n",
    "        self.data = raw_dataset[split].flatten()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = self.data.map(self._tokenize_batch, batched=True)\n",
    "        self.length = self.data[\"length\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src_ids = self.data[idx][\"input_ids\"]\n",
    "        tgt_ids = self.data[idx][\"labels\"]\n",
    "        return torch.tensor(src_ids), torch.tensor(tgt_ids)\n",
    "\n",
    "    def _tokenize_batch(self, example):\n",
    "        src_enc = tokenizer.encode_batch(example[\"translation.de\"])\n",
    "        src_ids, src_len = [], []\n",
    "        for e in src_enc:\n",
    "            src_ids.append(e.ids)\n",
    "            src_len.append(len(e.ids))\n",
    "        \n",
    "        tgt_enc = tokenizer.encode_batch(example[\"translation.en\"])\n",
    "        tgt_ids, tgt_len = [], []\n",
    "        for e in tgt_enc:\n",
    "            tgt_ids.append(e.ids)\n",
    "            tgt_len.append(len(e.ids))\n",
    "\n",
    "        seq_len = [max(len(a), len(b)) for a, b in zip(src_ids, tgt_ids)]\n",
    "        return {\"input_ids\": src_ids, \"labels\": tgt_ids, \"length\": seq_len}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c676a6f1-3c9c-4676-9b8a-e4bdd1741a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxTokensBucketSampler(Sampler):\n",
    "    def __init__(self, length, max_tokens=25000, shuffle=True):\n",
    "        self.length = length\n",
    "        self.max_tokens = max_tokens\n",
    "        self.shuffle = shuffle\n",
    "        self.bucket_batches = []\n",
    "        self._build_buckets()\n",
    "\n",
    "    def _build_buckets(self):\n",
    "        sorted_len_indices = sorted(range(len(self.length)), key=lambda i: self.length[i])\n",
    "        batch = []\n",
    "        tokens = 0\n",
    "        for idx in sorted_len_indices:\n",
    "            length = self.length[idx]\n",
    "            tokens += length\n",
    "            batch.append(idx)\n",
    "            if tokens >= self.max_tokens:\n",
    "                self.bucket_batches.append(batch)\n",
    "                batch = []\n",
    "                tokens = 0\n",
    "        if batch:\n",
    "            self.bucket_batches.append(batch)\n",
    "\n",
    "    def __iter__(self):\n",
    "        bucket_indices = list(range(len(self.bucket_batches)))\n",
    "        if self.shuffle:\n",
    "            random.shuffle(bucket_indices)\n",
    "        for idx in bucket_indices:\n",
    "            yield self.bucket_batches[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fdc2ea77-d9e1-4d9d-8d31-0eea96f70e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wmt_collate(batch, pad_token_id, batch_first=False):\n",
    "    src_batch, tgt_batch = zip(*batch)\n",
    "    src_padded = pad_sequence(src_batch, padding_value=pad_token_id, batch_first=batch_first)\n",
    "    tgt_padded = pad_sequence(tgt_batch, padding_value=pad_token_id, batch_first=batch_first)\n",
    "    return src_padded, tgt_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9dd7ffa-f1e7-4723-84b7-8c37935be985",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 10000)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_SEQ_LEN, MAX_TOKENS_PER_BATCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d1dba83-e431-4c36-adf0-b91c5a5dedae",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = WMT14Dataset(split=\"train\", tokenizer=tokenizer)\n",
    "train_sampler = MaxTokensBucketSampler(train_dataset.length, max_tokens=MAX_TOKENS_PER_BATCH, shuffle=True)\n",
    "train_loader = DataLoader(train_dataset, batch_sampler=train_sampler, collate_fn=lambda batch: wmt_collate(batch, PAD_ID))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ea4fbaa-21bf-4358-a0c8-bdbb3b341e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset = WMT14Dataset(split=\"validation\", tokenizer=tokenizer)\n",
    "eval_loader = DataLoader(eval_dataset, batch_size=32, shuffle=False, collate_fn=lambda batch: wmt_collate(batch, PAD_ID))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d057eb2-8de9-44fa-9742-b7b2d51f6e7d",
   "metadata": {},
   "source": [
    "### 3. Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d56a5032-c5db-4480-b3aa-1a41aae7477d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super().__init__()\n",
    "        den = torch.exp(- torch.arange(0, d_model, 2) * math.log(10000.0) / d_model)\n",
    "        pos = torch.arange(0, max_len).reshape(max_len, 1)\n",
    "        pos_embedding = torch.zeros((max_len, d_model))\n",
    "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
    "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
    "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)        \n",
    "        self.register_buffer('pos_embedding', pos_embedding)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.dropout(x + self.pos_embedding[:x.size(0), :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd87fc5e-78f7-4a14-a3ae-d2fafaaffece",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        pad_token_id,\n",
    "        d_model=512,\n",
    "        num_heads=8,\n",
    "        num_layers=6,\n",
    "        dim_ff=2048,\n",
    "        dropout=0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.pad_token_id = pad_token_id\n",
    "        self.d_model = d_model\n",
    "        self.shared_embed = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=d_model,\n",
    "            nhead=num_heads,\n",
    "            num_encoder_layers=num_layers,\n",
    "            num_decoder_layers=num_layers,\n",
    "            dim_feedforward=dim_ff,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        src,\n",
    "        tgt,\n",
    "        src_mask=None,\n",
    "        tgt_mask=None,\n",
    "        memory_mask=None,\n",
    "        src_key_padding_mask=None,\n",
    "        tgt_key_padding_mask=None,\n",
    "        memory_key_padding_mask=None\n",
    "    ):\n",
    "        src_mask, tgt_mask, src_key_padding_mask, tgt_key_padding_mask = self._create_masks(src, tgt, self.pad_token_id, src.device)\n",
    "        \n",
    "        src_emb = self.pos_encoder(self.shared_embed(src) * math.sqrt(self.d_model))\n",
    "        tgt_emb = self.pos_encoder(self.shared_embed(tgt) * math.sqrt(self.d_model))\n",
    "        output = self.transformer(src_emb,\n",
    "                                  tgt_emb,\n",
    "                                  src_mask=src_mask,\n",
    "                                  tgt_mask=tgt_mask,\n",
    "                                  memory_mask=memory_mask,\n",
    "                                  src_key_padding_mask=src_key_padding_mask,\n",
    "                                  tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "                                  memory_key_padding_mask=memory_key_padding_mask)\n",
    "        return self.fc_out(output)\n",
    "\n",
    "    def _create_masks(self, src, tgt, pad_id, device):\n",
    "        src_seq_len = src.shape[0]\n",
    "        tgt_seq_len = tgt.shape[0]\n",
    "    \n",
    "        src_mask = torch.zeros((src_seq_len, src_seq_len)).type(torch.bool).to(device)\n",
    "        tgt_mask = nn.Transformer.generate_square_subsequent_mask(tgt_seq_len).type(torch.bool).to(device)\n",
    "        \n",
    "        src_key_padding_mask = (src == pad_id).transpose(0, 1).to(device)\n",
    "        tgt_key_padding_mask = (tgt == pad_id).transpose(0, 1).to(device)\n",
    "\n",
    "        return src_mask, tgt_mask, src_key_padding_mask, tgt_key_padding_mask\n",
    "    \n",
    "    def encode(self, src, src_mask=None, src_key_padding_mask=None):\n",
    "        return self.transformer.encoder(self.pos_encoder(self.shared_embed(src) * math.sqrt(self.d_model)),\n",
    "                                        mask=src_mask,\n",
    "                                        src_key_padding_mask=src_key_padding_mask)\n",
    "\n",
    "    def decode(self, tgt, memory, tgt_mask=None, memory_mask=None, tgt_key_padding_mask=None, memory_key_padding_mask=None):\n",
    "        return self.transformer.decoder(self.pos_encoder(self.shared_embed(tgt) * math.sqrt(self.d_model)),\n",
    "                                        memory,\n",
    "                                        tgt_mask=tgt_mask,\n",
    "                                        memory_mask=memory_mask,\n",
    "                                        tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "                                        memory_key_padding_mask=memory_key_padding_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d52aece-e1e6-47c0-934b-912912fae79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InverseSqrtScheduler:\n",
    "    def __init__(self, model_size, tokens_per_step, warmup, optimizer):\n",
    "        self.model_size = model_size\n",
    "        self.tokens_per_step = tokens_per_step\n",
    "        self.warmup = warmup\n",
    "        self.optimizer = optimizer\n",
    "        self._step = 0\n",
    "        # Reference paper uses 25_000 tokens per step and 4_000 warm-up steps\n",
    "        self.factor = (self.tokens_per_step / 25_000) * (self.warmup / 4_000) ** 0.5\n",
    "\n",
    "    def step(self, scaler=None):\n",
    "        self._step += 1\n",
    "        lr = self.factor * (self.model_size ** -0.5) * min(self._step ** -0.5, self._step * (self.warmup ** -1.5))\n",
    "        for p in self.optimizer.param_groups:\n",
    "            p['lr'] = lr\n",
    "        if scaler:\n",
    "            scaler.step(self.optimizer)\n",
    "        else:\n",
    "            self.optimizer.step()\n",
    "\n",
    "    def lr_at_step(self, step):\n",
    "        lr = self.factor * (self.model_size ** -0.5) * min(step ** -0.5, step * (self.warmup ** -1.5))\n",
    "        return lr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea83ab51-1dbb-440a-ba7e-0d1f42be5d5b",
   "metadata": {},
   "source": [
    "### 4. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4161d528-512b-4f51-a639-1ef57421f86e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37000, 512, 10000, 0.1, 0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB_SIZE, D_MODEL, WARMUP_STEPS, LABEL_SMOOTHING, PAD_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c764e295-a7c7-4569-9880-d6827c3225a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/victor/miniconda3/envs/torch/lib/python3.12/site-packages/torch/nn/modules/transformer.py:382: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Preparing\n",
    "scaler = torch.amp.GradScaler(DEVICE)\n",
    "model = TransformerModel(VOCAB_SIZE, PAD_ID).to(DEVICE)\n",
    "opt = InverseSqrtScheduler(D_MODEL, MAX_TOKENS_PER_BATCH, WARMUP_STEPS, optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=LABEL_SMOOTHING, ignore_index=PAD_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9dc8b956-8f2e-48e3-9cf0-0b8a00cf8402",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 75000)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EVAL_STEPS, MAX_STEPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "943ad7cf-9fad-4437-b10d-33014b70ea70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started: 2025-08-04 05:45:59\n",
      "Step 1000: val loss = 7.5100, time per step = 0.3723 s, peak memory = 10.30 GB\n",
      "Step 2000: val loss = 6.8585, time per step = 0.3554 s, peak memory = 9.81 GB\n",
      "Step 3000: val loss = 6.5398, time per step = 0.3505 s, peak memory = 9.88 GB\n",
      "Step 4000: val loss = 6.3124, time per step = 0.3514 s, peak memory = 9.88 GB\n",
      "Step 5000: val loss = 6.0964, time per step = 0.3495 s, peak memory = 9.76 GB\n",
      "Step 6000: val loss = 5.7165, time per step = 0.3513 s, peak memory = 9.88 GB\n",
      "Step 7000: val loss = 5.1879, time per step = 0.3517 s, peak memory = 10.04 GB\n",
      "Step 8000: val loss = 4.7147, time per step = 0.3526 s, peak memory = 9.88 GB\n",
      "Step 9000: val loss = 4.2205, time per step = 0.3514 s, peak memory = 9.88 GB\n",
      "Step 10000: val loss = 4.2175, time per step = 0.3531 s, peak memory = 10.03 GB\n",
      "Step 11000: val loss = 4.0104, time per step = 0.3478 s, peak memory = 10.04 GB\n",
      "Step 12000: val loss = 3.8327, time per step = 0.3530 s, peak memory = 10.19 GB\n",
      "Step 13000: val loss = 3.8769, time per step = 0.3495 s, peak memory = 10.07 GB\n",
      "Step 14000: val loss = 3.7668, time per step = 0.3482 s, peak memory = 9.87 GB\n",
      "Step 15000: val loss = 3.8051, time per step = 0.3651 s, peak memory = 9.93 GB\n",
      "Step 16000: val loss = 3.7765, time per step = 0.3638 s, peak memory = 9.87 GB\n",
      "Step 17000: val loss = 3.7629, time per step = 0.3509 s, peak memory = 9.63 GB\n",
      "Step 18000: val loss = 3.6594, time per step = 0.3500 s, peak memory = 9.87 GB\n",
      "Step 19000: val loss = 3.6232, time per step = 0.3513 s, peak memory = 9.88 GB\n",
      "Step 20000: val loss = 3.7149, time per step = 0.3499 s, peak memory = 9.88 GB\n",
      "Step 21000: val loss = 3.5837, time per step = 0.3488 s, peak memory = 9.87 GB\n",
      "Step 22000: val loss = 3.6249, time per step = 0.3496 s, peak memory = 9.88 GB\n",
      "Step 23000: val loss = 3.5570, time per step = 0.3576 s, peak memory = 10.07 GB\n",
      "Step 24000: val loss = 3.5485, time per step = 0.3747 s, peak memory = 9.93 GB\n",
      "Step 25000: val loss = 3.5249, time per step = 0.3506 s, peak memory = 10.05 GB\n",
      "Step 26000: val loss = 3.5290, time per step = 0.3490 s, peak memory = 9.88 GB\n",
      "Step 27000: val loss = 3.4313, time per step = 0.3484 s, peak memory = 9.79 GB\n",
      "Step 28000: val loss = 3.4948, time per step = 0.3507 s, peak memory = 10.02 GB\n",
      "Step 29000: val loss = 3.4221, time per step = 0.3491 s, peak memory = 10.19 GB\n",
      "Step 30000: val loss = 3.4888, time per step = 0.3508 s, peak memory = 10.30 GB\n",
      "Step 31000: val loss = 3.4438, time per step = 0.3475 s, peak memory = 9.88 GB\n",
      "Step 32000: val loss = 3.4617, time per step = 0.3502 s, peak memory = 9.78 GB\n",
      "Step 33000: val loss = 3.4311, time per step = 0.3764 s, peak memory = 9.87 GB\n",
      "Step 34000: val loss = 3.4393, time per step = 0.3507 s, peak memory = 9.94 GB\n",
      "Step 35000: val loss = 3.4605, time per step = 0.3504 s, peak memory = 10.07 GB\n",
      "Step 36000: val loss = 3.4239, time per step = 0.3520 s, peak memory = 9.88 GB\n",
      "Step 37000: val loss = 3.4286, time per step = 0.3500 s, peak memory = 9.81 GB\n",
      "Step 38000: val loss = 3.4000, time per step = 0.3503 s, peak memory = 9.87 GB\n",
      "Step 39000: val loss = 3.4151, time per step = 0.3505 s, peak memory = 10.02 GB\n",
      "Step 40000: val loss = 3.3733, time per step = 0.3498 s, peak memory = 9.88 GB\n",
      "Step 41000: val loss = 3.3955, time per step = 0.3520 s, peak memory = 9.88 GB\n",
      "Step 42000: val loss = 3.3720, time per step = 0.3755 s, peak memory = 9.89 GB\n",
      "Step 43000: val loss = 3.3903, time per step = 0.3607 s, peak memory = 9.77 GB\n",
      "Step 44000: val loss = 3.3761, time per step = 0.3488 s, peak memory = 10.30 GB\n",
      "Step 45000: val loss = 3.3596, time per step = 0.3508 s, peak memory = 9.88 GB\n",
      "Step 46000: val loss = 3.3550, time per step = 0.3548 s, peak memory = 10.19 GB\n",
      "Step 47000: val loss = 3.3458, time per step = 0.3465 s, peak memory = 9.65 GB\n",
      "Step 48000: val loss = 3.3572, time per step = 0.3485 s, peak memory = 9.76 GB\n",
      "Step 49000: val loss = 3.3081, time per step = 0.3521 s, peak memory = 9.88 GB\n",
      "Step 50000: val loss = 3.3565, time per step = 0.3509 s, peak memory = 9.88 GB\n",
      "Step 51000: val loss = 3.3662, time per step = 0.3494 s, peak memory = 10.19 GB\n",
      "Step 52000: val loss = 3.3160, time per step = 0.3476 s, peak memory = 9.87 GB\n",
      "Step 53000: val loss = 3.3278, time per step = 0.3465 s, peak memory = 9.81 GB\n",
      "Step 54000: val loss = 3.3620, time per step = 0.3479 s, peak memory = 10.05 GB\n",
      "Step 55000: val loss = 3.3591, time per step = 0.3492 s, peak memory = 9.54 GB\n",
      "Step 56000: val loss = 3.3015, time per step = 0.3461 s, peak memory = 9.88 GB\n",
      "Step 57000: val loss = 3.3563, time per step = 0.3486 s, peak memory = 9.93 GB\n",
      "Step 58000: val loss = 3.3315, time per step = 0.3451 s, peak memory = 9.88 GB\n",
      "Step 59000: val loss = 3.2900, time per step = 0.4019 s, peak memory = 10.30 GB\n",
      "Step 60000: val loss = 3.2992, time per step = 0.3532 s, peak memory = 9.88 GB\n",
      "Step 61000: val loss = 3.2956, time per step = 0.3472 s, peak memory = 9.88 GB\n",
      "Step 62000: val loss = 3.3204, time per step = 0.3481 s, peak memory = 9.89 GB\n",
      "Step 63000: val loss = 3.2674, time per step = 0.3476 s, peak memory = 10.04 GB\n",
      "Step 64000: val loss = 3.3344, time per step = 0.3476 s, peak memory = 10.05 GB\n",
      "Step 65000: val loss = 3.2880, time per step = 0.3485 s, peak memory = 10.07 GB\n",
      "Step 66000: val loss = 3.2831, time per step = 0.3479 s, peak memory = 10.30 GB\n",
      "Step 67000: val loss = 3.2985, time per step = 0.3488 s, peak memory = 9.93 GB\n",
      "Step 68000: val loss = 3.2855, time per step = 0.3472 s, peak memory = 10.04 GB\n",
      "Step 69000: val loss = 3.2817, time per step = 0.3454 s, peak memory = 9.87 GB\n",
      "Step 70000: val loss = 3.3086, time per step = 0.3474 s, peak memory = 9.88 GB\n",
      "Step 71000: val loss = 3.3038, time per step = 0.3491 s, peak memory = 9.87 GB\n",
      "Step 72000: val loss = 3.2940, time per step = 0.3441 s, peak memory = 10.02 GB\n",
      "Step 73000: val loss = 3.3036, time per step = 0.3450 s, peak memory = 9.87 GB\n",
      "Step 74000: val loss = 3.2937, time per step = 0.3436 s, peak memory = 10.19 GB\n",
      "Step 75000: val loss = 3.2627, time per step = 0.3478 s, peak memory = 9.81 GB\n",
      "Training finished: 2025-08-04 13:10:31\n",
      "Overall peak memory was = 10.30 GB.\n"
     ]
    }
   ],
   "source": [
    "# Step control\n",
    "step = 0\n",
    "peak_memory_overall = 0\n",
    "val_losses, step_checkpoints = [], []\n",
    "best_val_loss = 1e6\n",
    "\n",
    "# # tell CUDA to start recording memory allocations\n",
    "# torch.cuda.memory._record_memory_history(enabled='all')\n",
    "\n",
    "print(\"Training started:\", datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "\n",
    "model.train()\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "start_time = time.time()\n",
    "\n",
    "while step < MAX_STEPS:\n",
    "    for src, tgt in train_loader:\n",
    "        # print(\"-- beginning\")\n",
    "        # torch.cuda.reset_peak_memory_stats()\n",
    "        # before = torch.cuda.memory_allocated()\n",
    "        \n",
    "        # Beggining allocation\n",
    "        src, tgt = src.to(DEVICE), tgt.to(DEVICE)\n",
    "        \n",
    "        # after = torch.cuda.memory_allocated()\n",
    "        # allocated = (after - before) / (1024**2)\n",
    "        # peak_memory = torch.cuda.max_memory_allocated() / (1024**2)\n",
    "        # print(f\"allocated: {allocated:.0f}M, peak: {peak_memory:.0f}M\")\n",
    "        # print(f\"src shape: {src.shape} and tgt shape: {tgt.shape}\")\n",
    "    \n",
    "        # Auto casting to float16 for memory reduction\n",
    "        with torch.amp.autocast(DEVICE, torch.float16):\n",
    "            # print(\"-- forward\")\n",
    "            # torch.cuda.reset_peak_memory_stats()\n",
    "            # before = torch.cuda.memory_allocated()\n",
    "            \n",
    "            # Forward pass\n",
    "            output = model(src, tgt[:-1, :])\n",
    "            \n",
    "            # after = torch.cuda.memory_allocated()\n",
    "            # allocated = (after - before) / (1024**2)\n",
    "            # peak_memory = torch.cuda.max_memory_allocated() / (1024**2)\n",
    "            # print(f\"allocated: {allocated:.0f}M, peak: {peak_memory:.0f}M\")\n",
    "            \n",
    "            # print(\"-- zero grad\")\n",
    "            # torch.cuda.reset_peak_memory_stats()\n",
    "            # before = torch.cuda.memory_allocated()\n",
    "            \n",
    "            # Zeroing the gradient\n",
    "            opt.optimizer.zero_grad()\n",
    "            \n",
    "            # after = torch.cuda.memory_allocated()\n",
    "            # allocated = (after - before) / (1024**2)\n",
    "            # peak_memory = torch.cuda.max_memory_allocated() / (1024**2)\n",
    "            # print(f\"allocated: {allocated:.0f}M, peak: {peak_memory:.0f}M\")\n",
    "            \n",
    "            # print(\"-- compute loss\")\n",
    "            # torch.cuda.reset_peak_memory_stats()\n",
    "            # before = torch.cuda.memory_allocated()\n",
    "            \n",
    "            # Loss computation\n",
    "            logits_flat = output.reshape(-1, output.shape[-1])\n",
    "            tgt_flat = tgt[1:, :].reshape(-1)\n",
    "            loss = criterion(logits_flat, tgt_flat)\n",
    "            \n",
    "            # after = torch.cuda.memory_allocated()\n",
    "            # allocated = (after - before) / (1024**2)\n",
    "            # peak_memory = torch.cuda.max_memory_allocated() / (1024**2)\n",
    "            # print(f\"allocated: {allocated:.0f}M, peak: {peak_memory:.0f}M\")\n",
    "        \n",
    "        # print(\"-- backward\")\n",
    "        # torch.cuda.reset_peak_memory_stats()\n",
    "        # before = torch.cuda.memory_allocated()\n",
    "    \n",
    "        # Loss backward\n",
    "        scaler.scale(loss).backward()\n",
    "        \n",
    "        # after = torch.cuda.memory_allocated()\n",
    "        # allocated = (after - before) / (1024**2)\n",
    "        # peak_memory = torch.cuda.max_memory_allocated() / (1024**2)\n",
    "        # print(f\"allocated: {allocated:.0f}M, peak: {peak_memory:.0f}M\")\n",
    "        \n",
    "        # print(\"-- optimizer step\")\n",
    "        # torch.cuda.reset_peak_memory_stats()\n",
    "        # before = torch.cuda.memory_allocated()\n",
    "        \n",
    "        # Optimizer step\n",
    "        opt.step(scaler)\n",
    "        \n",
    "        # after = torch.cuda.memory_allocated()\n",
    "        # allocated = (after - before) / (1024**2)\n",
    "        # peak_memory = torch.cuda.max_memory_allocated() / (1024**2)\n",
    "        # print(f\"allocated: {allocated:.0f}M, peak: {peak_memory:.0f}M\")\n",
    "        \n",
    "        # print(\"-- scaler update\")\n",
    "        # torch.cuda.reset_peak_memory_stats()\n",
    "        # before = torch.cuda.memory_allocated()\n",
    "        \n",
    "        # Scaler update\n",
    "        scaler.update()\n",
    "        \n",
    "        # after = torch.cuda.memory_allocated()\n",
    "        # allocated = (after - before) / (1024**2)\n",
    "        # peak_memory = torch.cuda.max_memory_allocated() / (1024**2)\n",
    "        # print(f\"allocated: {allocated:.0f}M, peak: {peak_memory:.0f}M\")\n",
    "        \n",
    "        # print(f\"***************** finish train step: {step}\")\n",
    "        # print()\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        step += 1\n",
    "        \n",
    "        if step % EVAL_STEPS == 0:\n",
    "            # Saving some stats\n",
    "            delta_time = (time.time() - start_time) / EVAL_STEPS\n",
    "            peak_memory = torch.cuda.max_memory_allocated() / (1024**3)\n",
    "            if peak_memory > peak_memory_overall:\n",
    "                peak_memory_overall = peak_memory\n",
    "            step_checkpoints.append(step)\n",
    "            \n",
    "            # Evaluation\n",
    "            model.eval()\n",
    "            val_loss = 0\n",
    "            with torch.no_grad():\n",
    "                for src, tgt in eval_loader:\n",
    "                    src, tgt = src.to(DEVICE), tgt.to(DEVICE)\n",
    "                    output = model(src, tgt[:-1, :])\n",
    "                    logits_flat = output.reshape(-1, output.shape[-1])\n",
    "                    tgt_flat = tgt[1:, :].reshape(-1)\n",
    "                    loss = criterion(logits_flat, tgt_flat)\n",
    "                    val_loss += loss.item()\n",
    "                    del output, loss\n",
    "                    torch.cuda.empty_cache()\n",
    "            avg_val_loss = val_loss / len(eval_loader)\n",
    "            val_losses.append(avg_val_loss)\n",
    "    \n",
    "            # Report every EVAL STEPS\n",
    "            print(f\"Step {step}: val loss = {val_losses[-1]:.4f}, time per step = {delta_time:.4f} s, peak memory = {peak_memory:.2f} GB\")\n",
    "            # Once training is done, we want to save out the model\n",
    "            if avg_val_loss < best_val_loss:\n",
    "                best_val_loss = avg_val_loss\n",
    "                torch.save(model.state_dict(), f\"models/wmt14_maxseqlen{MAX_SEQ_LEN}_maxtokens{MAX_TOKENS_PER_BATCH}_maxsteps{MAX_STEPS}_best.pt\")\n",
    "            \n",
    "            # Back to training\n",
    "            model.train()\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "            start_time = time.time()\n",
    "\n",
    "        # Stop batch iteration\n",
    "        if step >= MAX_STEPS:\n",
    "            break\n",
    "        \n",
    "# # Save a snapshot of the memory allocations\n",
    "# snapshot = torch.cuda.memory._snapshot()\n",
    "# with open(\"snapshot.pickle\", \"wb\") as f_out:\n",
    "#     pickle.dump(snapshot, f_out)\n",
    "# # Tell CUDA to stop recording memory allocations now\n",
    "# torch.cuda.memory._record_memory_history(enabled=None)\n",
    "\n",
    "print(\"Training finished:\", datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "print(f\"Overall peak memory was = {peak_memory_overall:.2f} GB.\")\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "torch.save(model.state_dict(), f\"models/wmt14_maxseqlen{MAX_SEQ_LEN}_maxtokens{MAX_TOKENS_PER_BATCH}_maxsteps{MAX_STEPS}_last.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c09538-a1de-4a84-84b1-678bde642302",
   "metadata": {},
   "source": [
    "### 5. Translating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f9e6973b-71c9-4269-9052-426c522a6231",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 10000, 75000)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_SEQ_LEN, MAX_TOKENS_PER_BATCH, MAX_STEPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a6486320-a9e9-406e-993f-f17a074c7723",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = torch.load(f\"models/wmt14_maxseqlen{MAX_SEQ_LEN}_maxtokens{MAX_TOKENS_PER_BATCH}_maxsteps{MAX_STEPS}_best.pt\", map_location=torch.device(DEVICE), weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5ffeee84-6239-4004-b952-5c854302e188",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dd4b29cf-2b90-48eb-8c6f-7875d6a41a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most probable next token\n",
    "def greedy_decode(model, src_ids, bos_token_id, eos_token_id, device='cuda'):\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Tokenizing the input\n",
    "        src_tensor = torch.tensor(src_ids).view(-1, 1).to(device)\n",
    "        num_tokens = src_tensor.shape[0]\n",
    "        max_len = num_tokens + 50\n",
    "        src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool).to(device)\n",
    "        \n",
    "        # Encoding\n",
    "        memory = model.encode(src_tensor, src_mask=src_mask)\n",
    "        # Starting with <START>\n",
    "        generated = torch.ones(1, 1).fill_(bos_token_id).type(torch.long).to(device)\n",
    "    \n",
    "        for cur_len in range(1, max_len):\n",
    "            tgt_mask = nn.Transformer.generate_square_subsequent_mask(cur_len).type(torch.bool).to(device)\n",
    "            output = model.decode(generated, memory, tgt_mask=tgt_mask).transpose(0, 1)        \n",
    "            logits = model.fc_out(output[:, -1])\n",
    "            _, next_token = torch.max(logits, dim=1)\n",
    "            next_token = next_token.item()\n",
    "            \n",
    "            generated = torch.cat([generated, torch.ones(1, 1).type(torch.long).fill_(next_token).to(device)], dim=0)\n",
    "            if next_token == eos_token_id:\n",
    "                break\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    return generated.view(1, -1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d2c1b860-179d-46bf-96f5-77ef061d135f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is an example text.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_ids = tokenizer.encode(\"Das ist ein Beispieltext.\").ids # This is an example text.\n",
    "preds = greedy_decode(model, src_ids, START_ID, END_ID, device=DEVICE)\n",
    "tokenizer.decode(preds.cpu().numpy(), skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7ec2c399-7a94-4489-931f-aa0ecded1aab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Nature is beautiful.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_ids = tokenizer.encode(\"Die Natur ist wunderschön.\").ids # Nature is beautiful.\n",
    "preds = greedy_decode(model, src_ids, START_ID, END_ID, device=DEVICE)\n",
    "tokenizer.decode(preds.cpu().numpy(), skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6cf8d825-24bb-46e8-b05b-0d2bfbf71827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from https://github.com/facebookresearch/XLM/blob/9e6f6814d17be4fe5b15f2e6c43eb2b2d76daeb4/src/model/transformer.py#L529\n",
    "def generate_beam(model, src_ids, pad_token_id, start_token_id, end_token_id, beam_size=4, alpha=0.6, early_stopping=True, device=\"cuda\"):\n",
    "    # check inputs\n",
    "    assert beam_size >= 1\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():    \n",
    "        src_ids = src_ids.to(device) # shape: (src_len, batch_size)\n",
    "        \n",
    "        # batch size\n",
    "        src_len, batch_size = src_ids.shape\n",
    "        max_len = src_len + 50\n",
    "        src_mask = torch.zeros((src_len, src_len), dtype=torch.bool, device=device) # shape: (src_len, src_len)\n",
    "        src_key_padding_mask = (src_ids == pad_token_id).transpose(0, 1).to(device) # shape: (batch_size, src_len)\n",
    "    \n",
    "        # encode source\n",
    "        memory = model.encode(src_ids, src_mask=src_mask, src_key_padding_mask=src_key_padding_mask) # shape: (src_len, batch_size, d_model)\n",
    "        \n",
    "        # repeating beam size times\n",
    "        memory = memory.repeat_interleave(beam_size, dim=1)                             # shape: (src_len, beam_size * batch_size, d_model)\n",
    "        src_key_padding_mask = src_key_padding_mask.repeat_interleave(beam_size, dim=0) # shape: (beam_size * batch_size, src_len)\n",
    "        \n",
    "        # generated sentences (batch with beam current hypotheses)\n",
    "        generated = torch.full((max_len, batch_size * beam_size), pad_token_id, dtype=torch.long, device=device)  # upcoming output\n",
    "        generated[0].fill_(start_token_id)\n",
    "        \n",
    "        # generated hypotheses\n",
    "        generated_hyps = [BeamHypotheses(beam_size, max_len, alpha, early_stopping) for _ in range(batch_size)]\n",
    "        \n",
    "        # scores for each sentence in the beam\n",
    "        beam_scores = torch.zeros((batch_size, beam_size), device=device)\n",
    "        beam_scores[:, 1:] = -1e9\n",
    "        beam_scores = beam_scores.view(-1)\n",
    "        \n",
    "        # current position\n",
    "        cur_len = 1\n",
    "        \n",
    "        # done sentences\n",
    "        done = [False for _ in range(batch_size)]\n",
    "        \n",
    "        while cur_len < max_len:\n",
    "            tgt_input = generated[:cur_len, :]\n",
    "            tgt_mask = nn.Transformer.generate_square_subsequent_mask(cur_len).type(torch.bool).to(device)\n",
    "            tgt_key_padding_mask = (tgt_input == pad_token_id).transpose(0, 1).to(device)\n",
    "            output = model.decode(tgt_input, memory,\n",
    "                                  tgt_mask=tgt_mask,\n",
    "                                  memory_mask=None,\n",
    "                                  tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "                                  memory_key_padding_mask=src_key_padding_mask)\n",
    "            logits = model.fc_out(output[-1])\n",
    "            scores = F.log_softmax(logits, dim=-1) # shape: (batch_size * beam_size, vocab_size)\n",
    "            vocab_size = scores.shape[-1]\n",
    "    \n",
    "            # select next words with scores\n",
    "            _scores = scores + beam_scores[:, None].expand_as(scores)             # shape: (batch_size * beam_size, vocab_size)\n",
    "            _scores = _scores.view(batch_size, beam_size * vocab_size)            # shape: (batch_size, beam_size * vocab_size)\n",
    "    \n",
    "            next_scores, next_words = torch.topk(_scores, 2 * beam_size, dim=1, largest=True, sorted=True)\n",
    "    \n",
    "            # next batch beam content\n",
    "            # list of (batch_size * beam_size) tuple(next hypothesis score, next word, current position in the batch)\n",
    "            next_batch_beam = []\n",
    "    \n",
    "            # for each sentence\n",
    "            for sent_id in range(batch_size):\n",
    "    \n",
    "                # if we are done with this sentence\n",
    "                done[sent_id] = done[sent_id] or generated_hyps[sent_id].is_done(next_scores[sent_id].max().item())\n",
    "                if done[sent_id]:\n",
    "                    next_batch_beam.extend([(0, pad_token_id, 0)] * beam_size)  # pad the batch\n",
    "                    continue\n",
    "    \n",
    "                # next sentence beam content\n",
    "                next_sent_beam = []\n",
    "    \n",
    "                # next words for this sentence\n",
    "                for idx, value in zip(next_words[sent_id], next_scores[sent_id]):\n",
    "    \n",
    "                    # get beam and word IDs\n",
    "                    beam_id = idx // vocab_size\n",
    "                    word_id = idx % vocab_size\n",
    "    \n",
    "                    # end of sentence, or next word\n",
    "                    if word_id == end_token_id or cur_len + 1 == max_len:\n",
    "                        generated_hyps[sent_id].add(generated[:cur_len, sent_id * beam_size + beam_id].clone(), value.item())\n",
    "                    else:\n",
    "                        next_sent_beam.append((value, word_id, sent_id * beam_size + beam_id))\n",
    "    \n",
    "                    # the beam for next step is full\n",
    "                    if len(next_sent_beam) == beam_size:\n",
    "                        break\n",
    "    \n",
    "                # update next beam content\n",
    "                if len(next_sent_beam) == 0:\n",
    "                    next_sent_beam = [(0, pad_token_id, 0)] * beam_size  # pad the batch\n",
    "                next_batch_beam.extend(next_sent_beam)\n",
    "\n",
    "            # sanity check / prepare next batch\n",
    "            beam_scores = beam_scores.new([x[0] for x in next_batch_beam])\n",
    "            beam_words = generated.new([x[1] for x in next_batch_beam])\n",
    "            beam_idx = torch.Tensor([x[2] for x in next_batch_beam]).long()\n",
    "    \n",
    "            # re-order batch and internal states\n",
    "            generated = generated[:, beam_idx]\n",
    "            generated[cur_len] = beam_words\n",
    "    \n",
    "            # update current length\n",
    "            cur_len = cur_len + 1\n",
    "    \n",
    "            # stop when we are done with each sentence\n",
    "            if all(done):\n",
    "                break\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # select the best hypotheses\n",
    "    tgt_len = torch.zeros(batch_size).long()\n",
    "    best = []\n",
    "\n",
    "    for i, hypotheses in enumerate(generated_hyps):\n",
    "        best_hyp = max(hypotheses.hyp, key=lambda x: x[0])[1]\n",
    "        tgt_len[i] = len(best_hyp) + 1  # +1 for the <EOS> symbol\n",
    "        best.append(best_hyp)\n",
    "\n",
    "    # generate target batch\n",
    "    decoded = torch.zeros(tgt_len.max().item(), batch_size).fill_(pad_token_id).long()\n",
    "    for i, hypo in enumerate(best):\n",
    "        decoded[:tgt_len[i] - 1, i] = hypo\n",
    "        decoded[tgt_len[i] - 1, i] = end_token_id\n",
    "    \n",
    "    return decoded.T\n",
    "\n",
    "class BeamHypotheses(object):\n",
    "\n",
    "    def __init__(self, n_hyp, max_len, length_penalty, early_stopping):\n",
    "        \"\"\"\n",
    "        Initialize n-best list of hypotheses.\n",
    "        \"\"\"\n",
    "        self.max_len = max_len - 1  # ignoring <BOS>\n",
    "        self.length_penalty = length_penalty\n",
    "        self.early_stopping = early_stopping\n",
    "        self.n_hyp = n_hyp\n",
    "        self.hyp = []\n",
    "        self.worst_score = 1e9\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Number of hypotheses in the list.\n",
    "        \"\"\"\n",
    "        return len(self.hyp)\n",
    "\n",
    "    def add(self, hyp, sum_logprobs):\n",
    "        \"\"\"\n",
    "        Add a new hypothesis to the list.\n",
    "        \"\"\"\n",
    "        score = sum_logprobs / len(hyp) ** self.length_penalty\n",
    "        if len(self) < self.n_hyp or score > self.worst_score:\n",
    "            self.hyp.append((score, hyp))\n",
    "            if len(self) > self.n_hyp:\n",
    "                sorted_scores = sorted([(s, idx) for idx, (s, _) in enumerate(self.hyp)])\n",
    "                del self.hyp[sorted_scores[0][1]]\n",
    "                self.worst_score = sorted_scores[1][0]\n",
    "            else:\n",
    "                self.worst_score = min(score, self.worst_score)\n",
    "\n",
    "    def is_done(self, best_sum_logprobs):\n",
    "        \"\"\"\n",
    "        If there are enough hypotheses and that none of the hypotheses being generated\n",
    "        can become better than the worst one in the heap, then we are done with this sentence.\n",
    "        \"\"\"\n",
    "        if len(self) < self.n_hyp:\n",
    "            return False\n",
    "        elif self.early_stopping:\n",
    "            return True\n",
    "        else:\n",
    "            return self.worst_score >= best_sum_logprobs / self.max_len ** self.length_penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b5066a-d476-4d6f-9129-931ecb155a27",
   "metadata": {},
   "source": [
    "### 6. BLEU Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7286dc59-780d-406b-9571-2afaf9e776ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = WMT14Dataset(split=\"test\", tokenizer=tokenizer)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, collate_fn=lambda batch: wmt_collate(batch, PAD_ID))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87a9ca9-475c-4e03-9c8b-4277a71e2334",
   "metadata": {},
   "source": [
    "### 6.1 Greedy decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5490d82c-fb18-493d-a8c7-3104defa4774",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Testing\n",
    "# greedy_preds = []\n",
    "# for src in test_dataset.data[\"translation.de\"]:\n",
    "#     src_ids = tokenizer.encode(src).ids\n",
    "#     greedy_preds.append(greedy_decode(model, src_ids, START_ID, END_ID, device=DEVICE))\n",
    "#     if len(greedy_preds) == 2:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "583a2b5e-6aef-4783-9c72-19308f0d6617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greedy decoding finished in 368.23 s\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "greedy_preds = []\n",
    "for src in test_dataset.data[\"translation.de\"]:\n",
    "    src_ids = tokenizer.encode(src).ids\n",
    "    preds = greedy_decode(model, src_ids, START_ID, END_ID, device=DEVICE)\n",
    "    decoded = tokenizer.decode(preds.cpu().numpy(), skip_special_tokens=True)\n",
    "    greedy_preds.append(decoded)\n",
    "\n",
    "delta_time = (time.time() - start_time)\n",
    "print(f\"Greedy decoding finished in {delta_time:.2f} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a112f352-ff18-42ac-9b30-42175b86be1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score: 26.80\n"
     ]
    }
   ],
   "source": [
    "bleu = sacrebleu.corpus_bleu(greedy_preds, [test_dataset.data[\"translation.en\"]])\n",
    "print(f\"BLEU score: {bleu.score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22934a39-2dc3-4b64-b5f8-2a7be4122081",
   "metadata": {},
   "source": [
    "### 6.2 Beam Search decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a3fbebdd-4880-4cf8-a1e8-917144495520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Testing\n",
    "# beam_preds = []\n",
    "# for src, _ in test_loader:\n",
    "#     decoded = generate_beam(model, src, PAD_ID, START_ID, END_ID, beam_size=4, alpha=0.6, device=DEVICE)\n",
    "#     beam_preds += tokenizer.decode_batch(decoded.cpu().numpy(), skip_special_tokens=True)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "09b021ab-ae5d-496a-a2b0-d7fc68483e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beam search decoding finished in 205.47 s\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "beam_preds = []\n",
    "for src, _ in test_loader:\n",
    "    preds = generate_beam(model, src, PAD_ID, START_ID, END_ID, beam_size=4, alpha=0.6, device=DEVICE)\n",
    "    decoded = tokenizer.decode_batch(preds.cpu().numpy(), skip_special_tokens=True)\n",
    "    beam_preds += decoded\n",
    "\n",
    "delta_time = (time.time() - start_time)    \n",
    "print(f\"Beam search decoding finished in {delta_time:.2f} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6816120d-2c39-4e98-ae76-1335c409297a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score: 27.50\n"
     ]
    }
   ],
   "source": [
    "bleu = sacrebleu.corpus_bleu(beam_preds, [test_dataset.data[\"translation.en\"]])\n",
    "print(f\"BLEU score: {bleu.score:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
